\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Still Spinning: The Case for Hard Drives in a Flashy World }
\author{csurbhi }
\date{May 2025}

\begin{document}

\maketitle

\section{Overview}
In this writeup we look at the details of NAND Flash SSDs, in particular how their cost depends on the type of NAND flash memory cell, their organization in parallel units, the interface used to communicate with the host, and the design of the on-device translation layer that manages the sequential only writes restriction of NAND flash memory.
This economics helps us understand why SMR drives are still lucrative despite the anecdotal evidence suggesting Flash drives will replace CMR drives and SMR drives will now replace tape, and that there is nothing more to it.



\section{Flash Memory Cells}
Fujio Masuoko developed the first “Fast Electrically erasable, electrically programmed ROM (EEPROM) cell” in 1984 \cite{masuoka1984new}. This memory cell could be overwritten in a "flash" as compared to the other EEPROM memories of the time that were erased using Ultraviolet light.  Writing to either the cell (termed Programming), had to be preceeded by an Erase operation that electrically readies the cell for writing. Once erased, programming a cell could trap electrons in it. Reading involved sensing the presence of trapped electrons in the cell. Erasing a cell could be performed only a limited number of time, as every erase degraded the insulators that made trapping of the electrons in the cell possible. The time to perform a erase, write and read is called as the Erase Cycle, write cycle and read cycle. Typically an erase cycle is much slower than the write cycle which is slower than the read cycle. The cells were arranged in parallel in rows and columns such that every bit could be erased individually \cite{masuoka1981bit}; this arrangement resembled a NOR gate and hence they were termed as NOR flash memory.

In 1987, Masuoko then introduced a new improved, denser cell that required similar Program/Erase/Read cycles as the NOR cells. However instead of being connected in parallel, 4 cells were connected in series, this arrangement resembled the NAND gate structure and was the memory cell was thus introduced as the NAND Flash memory\cite{masuoka1987new}. The way the NAND cells were composed,  2 of them could be erased together (and not individually), thus making the memory denser by reducing the necessary electronics.  While denser and faster to program and erase, these NAND cells were slower to read than the NOR cells containing 1 bit in a cell \cite{bez2003introduction}. So while the NOR cells became the basis of EEPROM memory that required faster reads and was rarely programmed, the NAND cells became the basis of the denser Solid state devices. Further strides in the NAND technology introduced denser cells, where each cell could store multiple bits. As the number of the bits per cell increases, the drive becomes denser, cheaper \cite{bauer2000multilevel}, however the number of erase cycles that each cell can tolerate lowers down, and the cost of read/write operation increases making the SSD slower \cite{kwon2011ftl}. Table \ref{ch2:tab:cell-comparison} compares the memory R/W performance, cost per GB, and power consumption based on the type of memory cells.  


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|>{\columncolor[gray]{0.8}}lr>{\columncolor[gray]{0.9}}rr>{\columncolor[gray]{0.9}}rr>{\columncolor[gray]{0.9}}rr>{\columncolor[gray]{0.9}}r}
\toprule 
\textbf{Attribute} & \textbf{SLC (SATA)} & \textbf{SLC (PCIe)} & \textbf{MLC (SATA)} & \textbf{MLC (PCIe)} & \textbf{TLC (SATA)} & \textbf{TLC (PCIe)} & \textbf{QLC (SATA)} & \textbf{QLC (PCIe)} \\
\midrule
\textbf{Number of bits per cell}     & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 \\
\textbf{Endurance (P/E cycles)}      & 50K–100K & 50K–100K & 3K–10K & 3K–10K & 1K–3K & 1K–3K & 100–1K & 100–1K \\
\textbf{Cost per GB}                 & Highest & Highest & High & High & Moderate & Moderate & Lowest & Lowest \\
\textbf{Power Consumption}          & Low & Low–Medium & Medium & Medium & Medium–High & Medium–High & High & High \\
\textbf{Random Read (4KB IOPS)}      & ~80K & ~100K & ~75K & ~90K & ~60K & ~85K & ~30K & ~60K \\
\textbf{Random Write (4KB IOPS)}     & ~70K & ~90K & ~45K & ~75K & ~15K & ~50K & ~5K & ~20K \\
\textbf{Sequential Read (MB/s)}     & ~550 & ~3000–5000 & ~550 & ~3000–5000 & ~500 & ~3000–5000 & ~450 & ~2500–4000 \\
\textbf{Sequential Write (MB/s)}    & ~500 & ~2000–4000 & ~450 & ~2000–4000 & ~300 & ~2000–4000 & ~150 & ~1000–2000 \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of SSD with different NAND Flash cell types with SATA and PCIe (NVMe) interfaces. A Single Level cell (SLC) stores a single bit, MLC cells (MLC) store two bits, Triple Level Cells (TLC) store three bits, and a Quad level cell stores four bits in each cell. }
\label{ch2:tab:cell-comparison}
\end{table}

\section{Internal structured of NAND Flash SSDs offer parallelism}
Solid state devices (SSD) comprise of NAND flash memory cells that are organized into pages that are the smallest unit of I/O. Multiple pages (typically 64 or 128) compose an Erase Block,  pages in an Erase block are erased sequentially as a unit \cite{jimenez2014wear}. 
<
1. multiple erase blocks -> planes, multiple planes -> die, multiple die->package. idependent i/o unit: die; hence parallel i/o possible. 
2. multiple planes in a die -> perform the same instruction on the same erase block, page within the plane.
status >
SSD density -> 3 dimensional packages.>


A NAND Flash controller is required to performs flash memory management, it typically runs on an embedded processor such as ARM Cortex-M \cite{marvell_ssd_mvss1331} or a more advance RISC-V processor. 
<
1. flash controller receives network packets from the host, interpreted as storage commands and executed to maximize bandwidth/iops through parallelism
2. connection between flash controller and a set of die -> called channel
3. connection between flash controller and one die -> way.
4. parallelism -> through ways. (one die -> erase, another die -> read/write.
5. wear leveling - worn pages - perform slower than unworn pages.
6. main task of flash controller - balance wear, maximise parallelism to maximise the bandwidth, error handling
7. parallelism achieved through threads - that run on distinct microprocessors. Cost goes up.
7. maximizing parallelism - subject of current research
>

\section{Protocol and Interface to SSD}
\subsection{AHCI over SATA}
1. SATA -> physical layer: connector details, signals. Link Layer: flow control of packets sent as signals. 
2. Does not provide manufacture agnostic command set and controller building details. This is provided by AHCI - allows SATA controllers to talk to each other independent of manufacturer. Standardizes command set, and provides specifictaion to host and controller to send commands and status (what registers are expected, where to write what, how initialization of the controller in the host is done).
3. SATA bandwidth 
4. Enough for HDDs. But not for SSDs.

\subsection{AHCI over PCIe}
1.SATA - connected to the Platform controller host (PCH) that connects to the CPU, simplex bus, slower.
2. PCIe - integrated with the CPU (can be distinct too), duplex, multi lanes - ie parallel communication on lanes.
arranges devices in a tree. The nodes are PCIe end devices - directly receive commands from the root. 
3. Makes interrupt handling faster - MSI-X interrupts.
4. Only controller needed in the device. 
5. Command set used is AHCI. The PCIe storage controller -> standardized by AHCI ( (what registers are expected, where to write what, how initialization of the controller in the host is done) - allows for multi manufacturer built controllers to work.
6. Poor idea - only SSDs used this. If you are anyway using PCIe, then why not the faster NVMe?

\subsection{NVMe over PCIe.}
1. PCIe is faster compared to SATA.
2. NVMe is faster compared to AHCI - better command queuing ( through multiple queues in the host for submission and completion of I/O commands)
3. SSD bandwidth and IOPs increases.

\begin{table}[]
\begin{tabular}{llllll}
Era         & Interface        & Type     & Max Speed      & Protocol Used  & Highlights                          \\
2003–2010s  & SATA + AHCI      & Serial   & $\sim$600 MB/s & ATA (via AHCI) & Native Command queueing, hot-plug, AHCI driver support  \\
2011+       & PCIe + NVMe      & Serial   & 3–16+ GB/s     & NVMe           & Built for SSDs, low latency, multiple queues \\
\end{tabular}
\caption{Interface-Protocol comparision}
\label{chp2:tab:if-protocol}
\end{table}

Cost of NVMe controller is 5-10 times higher than that of SATA controller.  NVMe SSDs require higher grade hardware to process all the parallelism offered by the NVMe protocol over SATA.
Cheapest NVMe SSD costs around (Crucial T500 4TB (Gen4)) -> USD 10 cents/GB
Cheapest SATA SSD costs around (Kingchuxing 2TB SATA) -> USD 5 cents/GB [their endurance is not published, a standard 3 year warranty exists. HDDs have a longer endurance. One metric we could use - (cents/GB per year) - getting this data needs more work.]
* Add more details - of these cheaper SSDs - what is the performance, what is the FTL Type, parallelism units etc.
* The cheapest SATA SSD is still 3 times that of the SMR Drive and atleast twice of CMR drives.

\section{Flash Translation layer}
The software that runs on a flash controller to perform Flash memory management is called the Flash Translation Layer (FTL) \cite{tavakkol2018mqsim}. 
The FTL takes decision of the placement of the incoming writes such that the sequential only writes restriction is observed. FTL also takes care of bad block management, wear leveling and exploiting parallel structures within the SSD to provide the best possible bandwidth to applications. Here, we only look at the data placement aspect of Flash Translation layer as that is the only aspect relevant to SMR drives. We look at the popularly discussed strategies for data placement now.
\section{Block Mapping}
1. data is written at the expected location witin an Erase Block.
2. Read - merge - write.
3. Amplified reads and writes. 

\section{Page Mapping}
1. Log structured writing.
2. Mapping table granularity -> page on the flash memory - usually 4KB.
3. GC -> the process of creating a whole free segment from hole riddled segment, take advantage of parallelism within the flash packages.
4. Erase blocks are reserved - for wear leveling that plays prominently during GC, and to allow application writes to complete.
5. Better than Block based Mapping, but for the size of the mapping table.
6. reduce the footprint of the mapping table -> two methods - dynamic loading of mapping table and second is reducing the number of entries in mapping table.


\section{Hybrid Log Block}
An obvious way to reduce the memory required for the mapping table is by actually reducing the number of entries in the mapping table. In this strategy, only a few Erase blocks are chosen to accept out of place writes; only these few erase blocks implement the LFS architecture and form the Log Blocks.  
Physically, the cache erase blocks and the data blocks are made of the same flash medium - both types of erase blocks can be exchanged as a result of this ie erase block A can be a cache block or data block. 

\subsection{Association of Data Blocks with Cache Blocks}
In the absence of mapping between the Data Blocks and the Cache blocks, a cache block can hold the data belonging to distinct Data blocks. Emptying such a cache block can result in merging the contents of cache block with as many distinct Data blocks as there are pages in the Cache block. The time required for emptying a cache block is dependent on the time required to read and write all these Data blocks that are undergoing a full merge. To reduce the long latency caused by the full merge of all these Data blocks, the number of data blocks that are mapped to a cache Erase block are restricted. 
1) 1 cache block - n data blocks - as n increases the cache utilization improves, but the cache eviction slows down.
2) 1:1 mapping - reduces utilization and cache eviction occurs frequently, average bandwidth reduces. Cache eviction improved.


\subsection{Reclaiming the Cache Blocks}
Emptying the pages in the cache blocks involves merging the contents of the cache with the contents of the respective data blocks. However, it can happen that the contents of the data block are invalid. Depending on the position of the valid pages in the cache erase blocks or data erase blocks there are three types of reclamation possible.
1) Full merge - Read merge write 
2) Partial Merge - The valid pages of the data erase block are copied to the end of the cache erase block, the cache erase block contains all the valid data now - it is marked as the data block by updating the mapping table of the Data blocks. The original data block is erased and marked free - it becomes the new cache block. This is possible - since physically the cache erase blocks and data blocks are the same.
3) Switch Merge - if all the valid pages of a data block are found in the cache erase blocks (ie all the pages in the data block are invalid) -> in this case no new data page copy is necessary. The cache erase block is marked as the data erase block!

\section{Conclusion - Speed vs Cost}
The speed and cost of a SSD also depends on
1. Cost and speed are directly proportional.
2. the internal parallelism  or channels within it; the more the channels, the more the micorprocessors, the more the flash memory packages and more the cost. A typical enterprise SSD has 8 to 16 channels, whereas a typical cheap consumer SSD has anywhere between 1 to 4 channels. 
2) Type of cells ->
2.a) SLC cell is expensive. on top of it, as the Erase blocks wear down, their Program/Erase cycles become longer by as much as a factor of three; this changes overall system performance \cite{desnoyers2010empirical}. Limited erase cycles thus has a cost and performance impact on SSDs. To compensate for the performance: Enterprise SSDs -> reserve more Erase blocks. 
2.b) QLC is cheaper - but has very poor endurance. To compensate, consumer QLC SSDs have more reserve blocks. If they don't they will underperform and die sooner.
3. Protocol and Interface to the SSD - NVMe SSDs are faster, but more expensive.
4. The architecture of Flash Translation Layer.
5. Cheaper SSDs still outperform Harddrives in terms of performance. However, currently and in the near future that will remain to be atleast 2-3 times more expensive.
6. Flash memory has the potential to become cheaper in the future - but right now are stuck in 3 dimensional packing. Future research can make it cheaper, but that is not immediate. In the meantime, Hard drive manufacturers are trying to make the hard drives denser and hence cheaper.










\end{document}
